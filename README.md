# Дашборд Волатильности BTC - Конвейер Данных

## 1. Обзор Проекта

Этот проект представляет собой полноценный ETL-конвейер для сбора, обработки и предоставления данных о волатильности Bitcoin. Система автоматически собирает часовые котировки с Binance, агрегирует их в дневные сессии (11:00–11:00 МСК), рассчитывает продвинутую статистику и предоставляет доступ к данным через REST API.

**Ключевые Технологии:**
*   **Язык:** Python
*   **Обработка данных:** Pandas
*   **Оркестрация:** Apache Airflow (через Docker)
*   **Веб-сервис:** FastAPI
*   **База данных (удаленный сервер):** MySQL / MariaDB
*   **Контейнеризация:** Docker, Docker Compose

---

## 2. Архитектура

Проект построен на модульной архитектуре с полной изоляцией зависимостей:
*   **Среда Airflow:** Отдельный Docker-образ `cofirazak/btc-airflow:latest`, отвечающий только за запуск и мониторинг задач.
*   **Среда Скриптов:** Отдельный Docker-образ `cofirazak/btc-scripts:latest`, содержащий всю бизнес-логику и необходимые библиотеки (`pandas`, `SQLAlchemy 2.x` и т.д.).
*   **База Данных:** Внешняя MariaDB/MySQL база данных.
*   **API Сервер:** Отдельный сервис на FastAPI, предоставляющий доступ к рассчитанным данным.

---

## 3. Установка и Запуск

Проект предлагает два пути установки: быстрый старт для конечного пользователя и полная установка для разработчика.

### 3.1. Быстрый Старт (для Пользователя)

Этот метод использует предварительно собранные Docker-образы с Docker Hub и является самым быстрым способом запустить систему.

**Требования:**
*   Docker
*   Docker Compose

**Конфигурация:**

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/cofirazak/btc-volatility-dashboard.git
    cd btc-volatility-dashboard
    ```

2.  **Создайте файл `.env`:**
    Создайте в корне проекта файл `.env` и заполните его данными для подключения к вашей базе данных. Используйте `.env.example` как шаблон.
    ```
    DB_USER=root
    DB_PASSWORD=your_secret_password
    DB_HOST=your_database_host_ip
    DB_PORT=your_database_port
    DB_NAME=crypto
    ```

3.  **Настройте права доступа (для macOS и Linux):**
    Эта команда добавит ID вашего текущего пользователя в `.env` файл. Это **критически важный шаг**, чтобы избежать проблем с правами доступа к файлам (логам и т.д.), которые Airflow будет создавать внутри Docker.
    ```bash
    echo -e "\nAIRFLOW_UID=$(id -u)" >> .env
    ```

**Запуск:**

Для первого запуска или для обновления до последней версии, выполните следующие команды:
```bash
# 1. Скачиваем последние версии всех образов с Docker Hub
docker compose pull

# 2. Запускаем все сервисы в фоновом режиме
docker compose up -d
```
Эта команда автоматически скачает или обновит готовые образы и запустит все необходимые сервисы.

**Остановка:**
Чтобы остановить все сервисы:
```bash
docker-compose down
```

---

### 3.2. Установка для Разработчика

Этот путь предназначен для разработчиков, которые хотят изменять исходный код и собирать Docker-образы самостоятельно.

**Установка:**
Следуйте шагам 1 и 2 из руководства "Быстрый Старт".

**Сборка и Запуск:**

1.  **Соберите образ для скриптов:**
    Эта команда создает Docker-образ со всеми Python-зависимостями для ваших скриптов.
    ```bash
    docker build -f scripts.Dockerfile -t your-docker-id/btc-scripts:latest .
    ```

2.  **Соберите и запустите систему Airflow:**
    Эта команда соберет кастомный образ для Airflow и запустит все сервисы. Убедитесь, что ваш `docker-compose.yaml` указывает на имя вашего образа.
    ```bash
    docker compose up -d --build
    ```

---

## 4. Доступ

*   **Интерфейс Airflow:** `http://localhost:8080` (логин: `airflow`, пароль: `airflow`)
*   **Документация API (Swagger):** `http://localhost:8000/docs`

---

## 5. Описание ETL-процесса

Конвейер состоит из двух основных DAG'ов в Airflow:

1.  **`btc_hourly_fetch`**
    *   **Расписание:** Каждый час.
    *   **Действие:** Запускает `backfill_script.py`, который проверяет последнюю запись в `klines_hourly` и догружает недостающие часовые свечи.
2.  **`btc_daily_aggregation`**
    *   **Расписание:** Каждый день в 11:05 МСК (08:05 UTC).
    *   **Действие:** Запускает `session_aggregator.py`, который читает новые часовые данные, пересчитывает последнюю сессию и рассчитывает всю статистику.

---

## 6. Описание API Эндпоинтов

*   **`GET /statistics`**
    *   **Описание:** Возвращает исторические данные по **закрытым** сессиям.
    *   **Параметр:** `days` (integer, default: 30) - количество последних дней для отображения.
    *   **Пример:** `http://localhost:8000/statistics?days=7`

*   **`GET /today`**
    *   **Описание:** Рассчитывает "на лету" и возвращает полную статистику для **текущей, еще не закрытой** сессии.

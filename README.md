# Дашборд Волатильности BTC - Конвейер Данных

## 1. Обзор Проекта

Этот проект представляет собой полноценный ETL-конвейер для сбора, обработки и предоставления данных о волатильности Bitcoin. Система автоматически собирает часовые котировки с Binance, агрегирует их в дневные сессии (11:00–11:00 МСК), рассчитывает продвинутую статистику и предоставляет доступ к данным через REST API.

**Ключевые Технологии:**
*   **Язык:** Python
*   **Обработка данных:** Pandas
*   **Оркестрация:** Apache Airflow (через Docker)
*   **Веб-сервис:** FastAPI
*   **База данных (удаленный сервер):** MySQL / MariaDB
*   **Контейнеризация:** Docker, Docker Compose

---

## 2. Архитектура

Проект построен на модульной архитектуре с полной изоляцией зависимостей:
*   **Среда Airflow:** Отдельный Docker-образ `cofirazak/btc-airflow:latest`, отвечающий только за запуск и мониторинг задач.
*   **Среда Скриптов:** Отдельный Docker-образ `cofirazak/btc-scripts:latest`, содержащий всю бизнес-логику и необходимые библиотеки (`pandas`, `SQLAlchemy 2.x` и т.д.).
*   **База Данных:** Внешняя MariaDB/MySQL база данных.
*   **API Сервер:** Отдельный сервис на FastAPI, предоставляющий доступ к рассчитанным данным.

---

## 3. Установка и Запуск

Проект предлагает два пути установки: быстрый старт для конечного пользователя и полная установка для разработчика.

### 3.1. Быстрый Старт (для Пользователя)

Этот метод использует предварительно собранные Docker-образы с Docker Hub и является самым быстрым способом запустить систему.

**Требования:**
*   Docker
*   Docker Compose

**Конфигурация:**

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/cofirazak/btc-volatility-dashboard.git
    cd btc-volatility-dashboard
    ```

2.  **Создайте файл `.env`:**
    Создайте в корне проекта файл `.env` и заполните его данными для подключения к вашей базе данных. Используйте `.env.example` как шаблон.
    ```
    DB_USER=root
    DB_PASSWORD=your_secret_password
    DB_HOST=your_database_host_ip
    DB_PORT=your_database_port
    DB_NAME=crypto
    ```

3.  **Настройте права доступа:**
    Эти настройки критически важны, чтобы избежать проблем с правами доступа к файлам и Docker-сокету.

    *   **a. Установите User ID (для macOS и Linux):**
        Эта команда добавит ID вашего текущего пользователя в `.env` файл. Это предотвратит проблемы с доступом к файлам (логам и т.д.), которые Airflow создает на вашем компьютере.
        ```bash
        echo -e "\nAIRFLOW_UID=$(id -u)" >> .env
        ```

    *   **b. Установите Docker Group ID (только для Linux):**
        При запуске на Linux может возникнуть ошибка доступа к Docker (`Permission denied`). Чтобы это исправить, нужно добавить ID группы `docker` в `.env` файл.
        *   **Узнайте ID группы:**
            ```bash
            getent group docker | cut -d: -f3
            ```
        *   **Добавьте в `.env`:**
            Добавьте полученное число в конец файла `.env`. Например, если команда вернула `999`:
            ```
            DOCK-ER_GROUP_ID=999
            ```
        Эта настройка не требуется для macOS.

**Запуск:**

Для первого запуска или для обновления до последней версии, выполните следующие команды:
```bash
# 1. Скачиваем последние версии всех образов с Docker Hub
docker compose pull

# 2. Запускаем все сервисы в фоновом режиме
docker compose up -d
```
Эта команда автоматически скачает или обновит готовые образы и запустит все необходимые сервисы.

**Остановка:**
Чтобы остановить все сервисы:
```bash
docker-compose down
```

---

### 3.2. Установка для Разработчика

Этот путь предназначен для разработчиков, которые хотят изменять исходный код и собирать Docker-образы самостоятельно.

**Установка:**
Следуйте шагам 1 и 2 из руководства "Быстрый Старт".

**Сборка и Запуск:**

1.  **Соберите образ для скриптов:**
    Эта команда создает Docker-образ со всеми Python-зависимостями для ваших скриптов.
    ```bash
    docker build -f scripts.Dockerfile -t your-docker-id/btc-scripts:latest .
    ```

2.  **Соберите и запустите систему Airflow:**
    Эта команда соберет кастомный образ для Airflow и запустит все сервисы. Убедитесь, что ваш `docker-compose.yaml` указывает на имя вашего образа.
    ```bash
    docker compose up -d --build
    ```

---

## 4. Доступ

*   **Интерфейс Airflow:** `http://localhost:8080` (логин: `airflow`, пароль: `airflow`)
*   **API Сервер:**
    *   **Статус API:** `http://localhost:8000` — вы увидите приветственное сообщение `{"status":"ok",...}`, подтверждающее, что API-сервер работает.
    *   **Интерактивная документация (Swagger UI):** `http://localhost:8000/docs` — здесь можно посмотреть все доступные эндпоинты и их описание.

**Как протестировать эндпоинты через Swagger:**
1.  Откройте страницу документации.
2.  Нажмите на интересующий вас эндпоинт (например, `GET /statistics`), чтобы развернуть его.
3.  В правом верхнем углу блока нажмите кнопку **"Try it out"**.
4.  При необходимости измените параметры (например, укажите `7` в поле `days`).
5.  Нажмите синюю кнопку **"Execute"**.
6.  Ниже вы увидите точный ответ от сервера, код ответа и заголовки.

---

## 5. Описание ETL-процесса

Конвейер состоит из двух основных DAG'ов в Airflow:

1.  **`btc_hourly_fetch`**
    *   **Расписание:** Каждый час.
    *   **Действие:** Запускает `backfill_script.py`, который проверяет последнюю запись в `klines_hourly` и догружает недостающие часовые свечи.
2.  **`btc_daily_aggregation`**
    *   **Расписание:** Каждый день в 11:05 МСК (08:05 UTC).
    *   **Действие:** Запускает `session_aggregator.py`, который читает новые часовые данные, пересчитывает последнюю сессию и рассчитывает всю статистику.

---

## 6. Описание API Эндпоинтов

*   **`GET /statistics`**
    *   **Описание:** Возвращает исторические данные по **закрытым** сессиям.
    *   **Параметр:** `days` (integer, default: 30) - количество последних дней для отображения.
    *   **Пример:** `http://localhost:8000/statistics?days=7`

*   **`GET /today`**
    *   **Описание:** Рассчитывает "на лету" и возвращает полную статистику для **текущей, еще не закрытой** сессии.

---

## 7. Управление задачами в Airflow

После первого запуска все задачи (DAGs) в Airflow находятся в состоянии "паузы" по умолчанию. Чтобы система начала собирать и обрабатывать данные, их необходимо активировать вручную.

**Как активировать задачи:**

1.  **Откройте интерфейс Airflow:** Перейдите по адресу `http://localhost:8080`.
2.  **Войдите в систему:** Используйте логин `airflow` и пароль `airflow`.
3.  **Перейдите на вкладку 'Dags':** В меню навигации слева выберите пункт 'Dags'. На этой странице вы увидите список из двух DAG'ов:
    *   `btc_hourly_fetch`
    *   `btc_daily_aggregation`
4.  **Активируйте каждый DAG:** Слева от имени каждого DAG'а находится переключатель. Нажмите на него, чтобы он стал активным (синим).

**Что делает каждый переключатель:**

*   **`btc_hourly_fetch`**:
    *   **Действие**: Активирует ежечасный сбор данных с Binance.
    *   **Результат**: После включения, Airflow будет запускать эту задачу в начале каждого часа. Она проверит последние записи в базе данных и догрузит недостающие часовые свечи.

*   **`btc_daily_aggregation`**:
    *   **Действие**: Активирует ежедневный расчет статистики.
    *   **Результат**: После включения, Airflow будет запускать эту задачу каждый день в 11:05 по московскому времени. Она закроет предыдущую торговую сессию, рассчитает для нее `Volatility` и `Range`, а также обновит всю скользящую статистику.

После активации Airflow начнет выполнять эти задачи согласно их расписанию.
